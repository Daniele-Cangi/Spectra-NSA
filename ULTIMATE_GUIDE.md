# ğŸ”¥ SPECTRA-NSA ULTIMATE - Quick Guide

> **âš ï¸ ALPHA STATUS**: Spectra-NSA architecture under active development. Breaking changes may occur. See [README.md](README.md) for details.

## ğŸ¯ What's New - Best of Both Worlds

### Your Modifications âœ…
- **M456 as DEFAULT** (458M params) - Perfect for Colab Pro
- **M600, M1B presets** - More scaling options
- **Component toggles** - Flexible ablation studies
- **Recall@K** - Built-in evaluation
- **Parameter estimation** - Debugging helper
- **Auto Drive Backup** - Every 6 checkpoints â†’ Google Drive

### Scientific Optimizations âœ…
- **Learnable fractal enrichment** - Adaptive depth mixing
- **Learnable spectral fusion** - Optimal blending
- **Temperature scheduling** - Cosine decay (0.07â†’0.05)
- **Scientific loss balancing** - Stable training
- **Integrated monitoring** - SOTA comparison & health checks
- **NaN debugging** - Automatic detection and recovery

---

## ğŸ“Š Size Presets Comparison

| Preset | Params | Hidden | Layers | Heads | Spectral | MaxLen | Best For |
|--------|--------|--------|--------|-------|----------|--------|----------|
| **M456** | ~458M | 1024 | 24 | 16 | 192 | 160 | **Colab Pro** âœ“ |
| M600 | ~600M | 1280 | 24 | 20 | 256 | 192 | Balanced |
| M700 | ~700M | 1536 | 24 | 24 | 384 | 256 | SOTA target |
| M1B | ~1.0B | 2048 | 32 | 32 | 512 | 320 | Research |

**Default**: M456 (optimal for Colab Pro A100 - VRAM safe)

---

## ğŸš€ Usage Examples

### 1. Default Training (M456, Colab-friendly)
```bash
python anomalous_embedding_ultimate.py --mode train --epochs 3
```

**Expected**:
- Training time: ~4-6h on A100 (fp32 mode)
- Total steps: ~36,750 (98k samples, 3 epochs)
- STS-B target: >0.825
- Memory usage: ~35GB (safe for 40GB A100)
- Checkpoints: Auto-save every 500 steps
- Drive backup: Every 6 checkpoints (3000 steps)

---

### 2. SOTA Training (M700)
```bash
python anomalous_embedding_ultimate.py --size M700 --mode train --epochs 3
```

**Expected**:
- Training time: ~12h on A100
- STS-B target: >0.840 (SOTA competitive)
- Memory usage: ~38GB

---

### 3. Research Scale (M1B)
```bash
python anomalous_embedding_ultimate.py --size M1B --mode train --epochs 3
```

**Warning**: Requires 80GB A100 or multi-GPU!

---

### 4. Ablation Study (Disable Components)
```bash
# No spectral attention
python anomalous_embedding_ultimate.py --no-spectral --mode train --epochs 1

# No anchor64
python anomalous_embedding_ultimate.py --no-anchor --mode train --epochs 1

# Minimal (no auxiliary components)
python anomalous_embedding_ultimate.py --no-spectral --no-anchor --no-bridge --no-matryoshka --mode train --epochs 1
```

**Use case**: Validate component contributions

---

### 5. Evaluation
```bash
# Quick eval (200 samples)
python anomalous_embedding_ultimate.py --size M456 --mode eval --checkpoint checkpoints/best_sts.pt

# Extract embeddings
python anomalous_embedding_ultimate.py --size M456 --mode extract --checkpoint checkpoints/best_sts.pt --texts "sample text" "another sample"
```

---

## ğŸ“ˆ Expected Performance by Size

| Size | STS-B Target | Training Time (A100) | Total Steps | Colab Pro? |
|------|--------------|---------------------|-------------|------------|
| M456 | >0.825 | 4-6h (fp32) | ~36,750 | âœ… Perfect |
| M600 | >0.835 | 6-8h | ~36,750 | âœ… Good |
| M700 | >0.840 | 8-10h | ~36,750 | âœ… Tight |
| M1B | >0.850 | 12-16h | ~36,750 | âš ï¸ Need 80GB |

---

## ğŸ›ï¸ Component Toggles Explained

### `--no-spectral`
- Disables spectral-entropy attention branch
- Expected impact: -1.5% STS
- Use to validate spectral contribution

### `--no-anchor`
- Disables anchor64 head + loss
- Expected impact: -0.3% STS
- Reduces model size slightly

### `--no-bridge`
- Disables semanticâ†”entity bridge loss
- Expected impact: -0.2% STS
- Simplifies loss landscape

### `--no-matryoshka`
- Disables matryoshka angular alignment (128â†”1536)
- Expected impact: -0.5% STS
- Faster training (one less loss term)

**Full ablation**:
```bash
python anomalous_embedding_ultimate.py \
    --no-spectral --no-anchor --no-bridge --no-matryoshka \
    --mode train --epochs 1
```
Expected: Baseline ~0.80 STS (vs 0.825 with all components)

---

## ğŸ’¾ Files Structure

```
checkpoints/
â”œâ”€â”€ best_sts.pt          # Best by STS score (use this!)
â”œâ”€â”€ best_val.pt          # Best by validation loss
â”œâ”€â”€ final.pt             # Final checkpoint
â””â”€â”€ step-*.pt            # Intermediate (every 500 steps)
```

**Checkpoint size**:
- M456: ~1.8GB
- M600: ~2.4GB
- M700: ~3.0GB
- M1B: ~4.5GB

---

## ğŸ” Monitoring Output

### During Training
```
[Epoch 1/3] Step 500 | Loss: 2.4567 | Temp: 0.0695
[MONITOR] Running quick STS evaluation at step 500...
[MONITOR] STS-B: 0.6892 | Target: 0.825 | Progress: 83.5%
âœ“ New best STS: 0.6892
```

### What to Watch
- **Loss decreasing**: ~3.0 â†’ ~1.0 over training
- **Temperature decay**: 0.07 â†’ 0.05 (cosine)
- **STS increasing**: Every 500 steps
- **Progress %**: Should reach >100% by end

### Warning Signs
```
âš ï¸ Gradient explosion detected
âš ï¸ Embedding collapse detected
âš ï¸ No improvement (5/5)
âš ï¸ EARLY STOPPING TRIGGERED
```

---

## ğŸ¯ Colab Pro Workflow

### 1. Setup (10 min)
```python
# In Colab notebook
!pip install transformers datasets accelerate scipy

from google.colab import drive
drive.mount('/content/drive')

# Upload anomalous_embedding_ultimate.py to Drive
# Copy to Colab runtime
!cp /content/drive/MyDrive/anomalous_embedding_ultimate.py /content/
```

### 2. Train M456 (6-8h)
```python
!python anomalous_embedding_ultimate.py --mode train --epochs 3
# Go to sleep, come back to trained model
```

### 3. Auto-Backup (Already Enabled!)
Checkpoints are **automatically backed up** to Google Drive every 6 saves:
```
[CHECKPOINT] Saved â†’ checkpoints/step-3000.pt
[DRIVE BACKUP] Checkpoint #6 â†’ /content/drive/MyDrive/anomalous_checkpoints/step-3000.pt
```

Manual backup (if needed):
```python
!cp -r checkpoints /content/drive/MyDrive/anomalous_checkpoints/
print("âœ“ Manual backup complete")
```

### 4. Evaluate
```python
!python anomalous_embedding_ultimate.py --mode eval --checkpoint checkpoints/best_sts.pt
# Expected: STS-B > 0.825
```

---

## ğŸ§ª Quick Experiment Matrix

| Experiment | Command | Expected STS | Time |
|------------|---------|--------------|------|
| **Full M456** | `--mode train` | 0.825 | 6-8h |
| No spectral | `--no-spectral` | 0.810 | 6h |
| No anchor | `--no-anchor` | 0.822 | 6h |
| Minimal | `--no-spectral --no-anchor --no-bridge --no-matryoshka` | 0.800 | 5h |
| **Full M700** | `--size M700` | 0.840 | 12h |

---

## ğŸ† Success Criteria

### M456 (Default)
- âœ… STS-B > 0.825 (competitive with BGE-base)
- âœ… Training completes <8h on A100
- âœ… Fits in Colab Pro (40GB)
- âœ… No crashes or OOM

### M700 (SOTA Target)
- âœ… STS-B > 0.840 (competitive with GTE-large)
- âœ… NDCG@10 > 0.420
- âœ… Recall@1 > 0.550
- âœ… Training completes <12h on A100

### M1B (Research)
- âœ… STS-B > 0.850 (SOTA-beating)
- âœ… Training completes <20h on 80GB

---

## ğŸ“ Troubleshooting

### "Unknown size: M456"
â†’ Check you're using `anomalous_embedding_ultimate.py` (not old version)

### Out of Memory (M456)
â†’ Already optimized! Current: `batch_train=8`, `max_length=160`
â†’ If still OOM: reduce `batch_train=6` or `max_length=128`

### Out of Memory (M700)
â†’ Expected on non-A100. Use M456 instead or reduce batch to 6.

### NaN/Inf in Loss
â†’ **Auto-recovery enabled**: `torch.nan_to_num` fallback active
â†’ Check `[NaN DEBUG]` messages for source component
â†’ FP32 mode enabled for debugging (more stable than fp16)

### Training too slow
â†’ Verify A100: `!nvidia-smi` should show "A100-SXM4-40GB"

### STS not improving
â†’ Check component toggles: Running with `--no-spectral --no-anchor` will reduce performance

---

## ğŸ“ Advanced Tips

### 1. Custom Size (Advanced)
Edit config in script:
```python
cfg.hidden_size = 1152  # Custom
cfg.num_layers = 20
cfg.num_heads = 18
```

### 2. Faster Iteration (Debug)
```bash
python anomalous_embedding_ultimate.py --epochs 1 --mode train
# Quick 1-epoch test
```

### 3. Gradient Checkpointing (Memory Optimization) âœ…
```bash
# Enable gradient checkpointing to reduce VRAM usage
python anomalous_embedding_ultimate.py --mode train --gradient-checkpointing

# Useful for larger models on smaller GPUs
python anomalous_embedding_ultimate.py --size M700 --gradient-checkpointing --mode train

# Trade-off: ~20% slower training but ~30% less VRAM
```

### 4. Multi-GPU Training âœ…
```bash
# Configure Accelerate (first time only)
accelerate config

# Launch multi-GPU training
accelerate launch anomalous_embedding_ultimate.py --mode train --epochs 3

# Resume multi-GPU training
accelerate launch anomalous_embedding_ultimate.py --mode train --resume checkpoints/step-5000.pt

# Multi-GPU with gradient checkpointing
accelerate launch anomalous_embedding_ultimate.py --mode train --gradient-checkpointing --size M1B
```

**Benefits**:
- Automatic data parallelism
- Scaled effective batch size
- Synchronized gradients
- Main process handles checkpointing

---

## ğŸ“Š Comparison Table

| Feature | Original | Optimized | **Ultimate** |
|---------|----------|-----------|--------------|
| Presets | M300, M700 | M300, M700 | **M456, M600, M700, M1B** |
| Default | M300 | M700 | **M456** (Colab) |
| Fractal | Fixed | Learnable | **Learnable** |
| Spectral Fusion | Fixed | Learnable | **Learnable** |
| Temperature | Fixed | Scheduled | **Scheduled** |
| Toggles | No | No | **Yes** (4 toggles) |
| Loss Balance | Empirical | Scientific | **Scientific** |
| Monitoring | Basic | Integrated | **Integrated** |
| Param Estimate | No | No | **Yes** |

---

## ğŸš€ Bottom Line

**ULTIMATE = Your Smart Modifications + My Scientific Optimizations**

### Perfect for Colab Pro:
```bash
python anomalous_embedding_ultimate.py --mode train --epochs 3
```

- âœ… M456 default (458M) fits perfectly
- âœ… 6-8h training time
- âœ… Target STS >0.825
- âœ… All optimizations enabled
- âœ… Component toggles for experiments
- âœ… Integrated monitoring
- âœ… Auto early-stopping

### For SOTA Push:
```bash
python anomalous_embedding_ultimate.py --size M700 --mode train --epochs 3
```

- âœ… M700 (700M) for max quality
- âœ… 12h training time
- âœ… Target STS >0.840 (competitive with GTE)
- âœ… Still fits in Colab Pro (tight)

---

**File**: `anomalous_embedding_ultimate.py` (~45KB)
**Status**: âš ï¸ **ALPHA** - Architecture under active development
**License**: Apache 2.0
**Best for**: Research & experimentation on Colab Pro with M456 default

ğŸ¯ **Ready to train!**

---

## âš ï¸ Important Notes

1. **Alpha Software**: Breaking changes may occur. Pin versions for reproducibility.
2. **FP32 Mode**: Currently forced for NaN debugging. Slower but more stable.
3. **Drive Backup**: Requires Google Drive mounted at `/content/drive/MyDrive`
4. **Memory**: Tested on A100 40GB. Other GPUs may require adjustments.
5. **License**: Apache 2.0 - See [README.md](README.md) for full terms.

ğŸ“š **See [README.md](README.md) for complete documentation and citation info.**
